# Naïve Bayes

KNN and Decision Tree give us exactly the classification, and Naïve Bayes will give us 
a guess of the classification about the class and assign a probability estimate to that 
best guess by Bayesian rules.



Summary 

-Using probabilities can sometimes be more effective than using hard rules for classification. 
Bayesian probability and Bayes’ rule gives us a way to estimate unknown probabilities from known values. 
You can reduce the need for a lot of data by assuming conditional independence among the features in your data.

--The assumption we make is the independence of ach data. We know this assumption is a little simple. 
That’s why it’s known as naïve Bayes. Despite its incorrect assumptions, naïve Bayes is effective at classification. 

--Underflow is one problem that can be addressed by using the logarithm of probabilities. 
The bag-of-words model is an improvement on the set-of-words model when approaching document classification. 
There are several other improvements, such as removing stop words. 
